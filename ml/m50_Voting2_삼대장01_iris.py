from dataclasses import dataclass
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris,load_breast_cancer
from sklearn.model_selection import KFold,cross_val_score,GridSearchCV,StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, r2_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier,VotingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from xgboost import XGBClassifier,XGBRFRegressor
from lightgbm import LGBMClassifier, LGBMRegressor
from catboost import CatBoostClassifier, CatBoostRegressor
import warnings
warnings.filterwarnings(action="ignore")

#.1데이터
datasets = load_iris()

df = pd.DataFrame(datasets.data, columns=datasets.feature_names)
print(df.head(7))  #head는 기본 5개 출력

x_train ,x_test, y_train, y_test = train_test_split(
    datasets.data, datasets.target, random_state=123, train_size=0.8, shuffle=True, stratify=datasets.target)



Scaler = StandardScaler() #Bagging 할때 스케일러 필수 
x_train = Scaler.fit_transform(x_train)
x_test = Scaler. transform(x_test)

#.2 모델
xg = XGBClassifier(max_depth=3,
                   learning_rate=0.1, n_estimators=100, n_jobs=-1,
                   objective='multi:softproba', 
                   num_class=3, reg_alpha=0.1, reg_lambda=0.1,
                   subsample=0.8, )

lg = LGBMClassifier(max_depth=3,
                   learning_rate=0.1, n_estimators=100, n_jobs=-1,
                   num_class=3, reg_alpha=0.1, reg_lambda=0.1,
                   subsample=0.8, )

cat = CatBoostClassifier(max_depth=3,
    )#(verbose=False)#catboost vervose가 많음 ! 그래서 다른모델이랑 성능비교 시에는 주석처리

#voting 은 hard &soft가있음 #estimators= 두개이상은 리스트로 넣어줘야함
model = VotingClassifier(estimators=[('xg', xg), ('cat', cat),("lg", lg)], voting='soft') 

#3. 평가예측
model.fit(x_train,y_train)

#4. 평가,예측
y_predict = model.predict(x_test)
print(model.score(x_test,y_test))

score = accuracy_score(y_test,y_predict)
print("보팅결과 : ", round(score,4 ))

# 보팅결과 :  0.9912

classifier = [cat,xg, lg,]

for model in classifier:  #model2는 모델이름 # 
    model.fit(x_train,y_train)
    y_predict = model.predict(x_test)
    score2 = accuracy_score(y_test,y_predict)
    class_name = model.__class__.__name__  #<모델이름 반환해줌 
    print("{0}정확도 : {1:.4f}".format(class_name, score2)) # f = format
    
print("보팅결과 : ", round(score,4 ))
    
# XGBClassifier정확도 : 0.9912
# CatBoostClassifier정확도 : 0.9000
# XGBClassifier정확도 : 0.9333
# LGBMClassifier정확도 : 0.9333
# 보팅결과 :  0.9333