import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN
from tensorflow.keras.layers import Bidirectional
import pandas as p

#1.데이터
datasets = np.array([1,2,3,4,5,6,7,8,9,10])
# x, y = zip(['1,2,3', 4], ['4,5,6', 7], ['7,8,9', 10])

x = np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6],[5,6,7],[6,7,8],[7,8,9]])
y = np.array([4,5,6,7,8,9,10])

print(x.shape, y.shape)  # x(7 , 3) y(7,)

# input_shape = (행 , 열 , 몇개씩 자르는지! Rnn 쉐이프 3 차원 )
x = x.reshape
# print(x.shape)#(7,3,1,)

#2. 모델구성                         #cnn = filters  #dnn = units rnn = SimpleRNN 
model = Sequential()                #[batch, timesteps, feature]
model.add(SimpleRNN(20 ,input_shape = (3, 1),return_sequences=True))
model.add(Bidirectional(SimpleRNN(5)))  #삼차원으로 들어감)
model.add(Dense(3, activation='relu'))    #(RNN 여기서2차원으로 바뀜)
model.add(Dense(1))
model.summary()

# _
#  Layer (type)                Output Shape              Param #  

# =================================================================
#  simple_rnn (SimpleRNN)      (None, 3, 10)             120      



#  bidirectional (Bidirectiona  (None, 10)               160      

#  l)



#  dense (Dense)               (None, 3)                 33       



#  dense_1 (Dense)             (None, 1)                 4        



# =================================================================
# Total params: 317


#units * (feature + bisa + units) = parms

'''
#3.컴파일 훈련
model.compile(loss ='mse',optimizer='adam')
model.fit(x,y, epochs=1000)

#4. 평가 예측
loss = model.evaluate(x,y)
y_pred = np.array([8,9,10]).reshape(1,3,1) #[[[8],[9],[10]]]
result = model.predict(y_pred)  #평가예측에서 똑같이 맟춰서
print('loss:',loss)
print('[8,9,10]의 결과:',result)  #RNN input_shape 에서 들어간 차원을 야함
'''